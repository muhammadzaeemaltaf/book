"""
Gemini API client for the Physical AI textbook platform.
This module provides utilities for interacting with the Google Gemini API.
"""
from typing import Optional, Dict, Any, List, AsyncGenerator
import google.generativeai as genai
from google.generativeai import GenerativeModel
from google.generativeai.types import GenerationConfig
import logging
from pydantic import BaseModel

from .config import settings


logger = logging.getLogger(__name__)


class GeminiClient:
    """
    Client for interacting with Google Gemini API.
    """

    def __init__(self):
        """
        Initialize the Gemini client with configuration from settings.
        """
        # Configure the API key
        api_key = settings.google_api_key or settings.gemini_api_key
        if not api_key:
            raise ValueError("Google API key or Gemini API key is required for Gemini client")

        genai.configure(api_key=api_key)

        # Initialize the model
        self.model_name = settings.gemini_model
        self.model: GenerativeModel = genai.GenerativeModel(self.model_name)

        # Default generation configuration
        self.default_config = GenerationConfig(
            temperature=0.7,
            max_output_tokens=2048,
            top_p=0.9,
            top_k=40
        )

    async def generate_content(
        self,
        prompt: str,
        generation_config: Optional[GenerationConfig] = None,
        safety_settings: Optional[Dict] = None
    ) -> str:
        """
        Generate content using the Gemini model.

        Args:
            prompt: The input prompt for content generation
            generation_config: Optional generation configuration
            safety_settings: Optional safety settings

        Returns:
            Generated content as a string
        """
        try:
            if generation_config is None:
                generation_config = self.default_config

            response = await self.model.generate_content_async(
                prompt,
                generation_config=generation_config,
                safety_settings=safety_settings
            )

            if response.candidates and len(response.candidates) > 0:
                return response.candidates[0].content.parts[0].text
            else:
                raise Exception("No content generated by Gemini model")

        except Exception as e:
            logger.error(f"Error generating content with Gemini: {str(e)}")
            raise

    async def generate_content_stream(
        self,
        prompt: str,
        generation_config: Optional[GenerationConfig] = None,
        safety_settings: Optional[Dict] = None
    ) -> AsyncGenerator[str, None]:
        """
        Generate content using the Gemini model with streaming.

        Args:
            prompt: The input prompt for content generation
            generation_config: Optional generation configuration
            safety_settings: Optional safety settings

        Yields:
            Content chunks as they are generated
        """
        try:
            if generation_config is None:
                generation_config = self.default_config

            response = await self.model.generate_content_async(
                prompt,
                generation_config=generation_config,
                safety_settings=safety_settings,
                stream=True
            )

            for chunk in response:
                if chunk.candidates and len(chunk.candidates) > 0:
                    text = chunk.candidates[0].content.parts[0].text
                    yield text

        except Exception as e:
            logger.error(f"Error generating content stream with Gemini: {str(e)}")
            raise

    async def embed_content(
        self,
        text: str,
        task_type: str = "RETRIEVAL_DOCUMENT",
        title: Optional[str] = None
    ) -> List[float]:
        """
        Generate embeddings for the given text using Gemini's embedding model.

        Args:
            text: The text to embed
            task_type: The type of task (RETRIEVAL_DOCUMENT, RETRIEVAL_QUERY, etc.)
            title: Optional title for the text

        Returns:
            List of embedding values
        """
        try:
            result = await genai.embed_content_async(
                model=settings.embedding_model,
                content=text,
                task_type=task_type,
                title=title
            )
            return result['embedding']
        except Exception as e:
            logger.error(f"Error generating embedding with Gemini: {str(e)}")
            raise

    async def batch_embed_contents(
        self,
        texts: List[str],
        task_type: str = "RETRIEVAL_DOCUMENT"
    ) -> List[List[float]]:
        """
        Generate embeddings for multiple texts.

        Args:
            texts: List of texts to embed
            task_type: The type of task (RETRIEVAL_DOCUMENT, RETRIEVAL_QUERY, etc.)

        Returns:
            List of embedding vectors
        """
        try:
            results = await genai.embed_content_async(
                model=settings.embedding_model,
                content=texts,
                task_type=task_type
            )
            return results['embeddings']
        except Exception as e:
            logger.error(f"Error generating batch embeddings with Gemini: {str(e)}")
            raise


class AIAgent:
    """
    Higher-level AI agent that uses the Gemini client for various tasks.
    """

    def __init__(self):
        self.gemini_client = GeminiClient()

    async def generate_summary(
        self,
        content: str,
        max_length: int = 500,
        style: str = "concise"
    ) -> str:
        """
        Generate a summary of the given content.

        Args:
            content: The content to summarize
            max_length: Maximum length of the summary
            style: Style of the summary ("concise", "detailed", "technical")

        Returns:
            Generated summary
        """
        prompt = f"""
        Please provide a {style} summary of the following content.
        The summary should be no longer than {max_length} words.

        Content: {content}

        Summary:
        """

        return await self.gemini_client.generate_content(prompt)

    async def personalize_content(
        self,
        original_content: str,
        user_profile: Dict[str, Any],
        target_audience: str = "mixed"
    ) -> str:
        """
        Personalize content based on user profile.

        Args:
            original_content: The original content to personalize
            user_profile: User's technical background and preferences
            target_audience: Target audience level ("beginner", "intermediate", "advanced", "mixed")

        Returns:
            Personalized content
        """
        profile_str = ", ".join([f"{key}: {value}" for key, value in user_profile.items()])

        prompt = f"""
        Personalize the following content for a user with this technical background:
        {profile_str}

        Target audience level: {target_audience}

        Original content: {original_content}

        Personalized content (adapt complexity, examples, and explanations based on the user's background):
        """

        return await self.gemini_client.generate_content(prompt)

    async def answer_question(
        self,
        question: str,
        context: Optional[str] = None,
        max_tokens: int = 1000
    ) -> str:
        """
        Answer a question, optionally with context.

        Args:
            question: The question to answer
            context: Optional context to consider when answering
            max_tokens: Maximum number of tokens in the response

        Returns:
            Answer to the question
        """
        if context:
            prompt = f"""
            Based on the following context, answer the question:

            Context: {context}

            Question: {question}

            Answer:
            """
        else:
            prompt = f"""
            Answer the following question:

            Question: {question}

            Answer:
            """

        config = GenerationConfig(
            temperature=0.3,  # Lower temperature for more factual answers
            max_output_tokens=max_tokens,
            top_p=0.8,
            top_k=30
        )

        return await self.gemini_client.generate_content(prompt, generation_config=config)


# Global instance
gemini_client: Optional[GeminiClient] = None
ai_agent: Optional[AIAgent] = None


def get_gemini_client() -> GeminiClient:
    """
    Get the global Gemini client instance, creating it if it doesn't exist.
    """
    global gemini_client
    if gemini_client is None:
        gemini_client = GeminiClient()
    return gemini_client


def get_ai_agent() -> AIAgent:
    """
    Get the global AI agent instance, creating it if it doesn't exist.
    """
    global ai_agent
    if ai_agent is None:
        ai_agent = AIAgent()
    return ai_agent